{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wasineer-dev/braid/blob/develop/inputSet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z-iHVgrLLWI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from time import time as timer\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "MAX_ITERATION = 20\n",
        "\n",
        "class CMeanFieldAnnealing:\n",
        "\n",
        "    def __init__(self, Nproteins, Nk):\n",
        "        self.lstExpectedLikelihood = []\n",
        "        self.mIndicatorQ = np.zeros((Nproteins, Nk), dtype=float)\n",
        "\n",
        "    def tf_annealing(self, mix_p, mObservationG, Nproteins, Nk, psi):\n",
        "\n",
        "        matA = tf.convert_to_tensor(mObservationG.mTrials - mObservationG.mObserved, dtype=tf.float32)\n",
        "        matB = tf.convert_to_tensor(psi*mObservationG.mObserved, dtype=tf.float32)\n",
        "        tfArray = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "        for i in range(Nproteins):\n",
        "            tfArray = tfArray.write(i, self.mIndicatorQ[i])\n",
        "        \n",
        "        gamma = 1000.0\n",
        "        nIteration = 0\n",
        "        while(nIteration < MAX_ITERATION and gamma > 1.0):\n",
        "            for i in range(Nproteins):        \n",
        "                tQ = tfArray.stack()\n",
        "                fn_out = tf.tensordot(matA[i], tQ, axes=1) \n",
        "                fp_out = tf.tensordot(matB[i], 1.0 - tQ, axes=1)\n",
        "\n",
        "                mLogLikelihood = fn_out + fp_out\n",
        "                tfArray = tfArray.write(i, tf.nn.softmax(-gamma*mLogLikelihood))\n",
        "\n",
        "            nIteration += 1\n",
        "            gamma = gamma - 100.0\n",
        "        print(\"Initialize with MFA: num. iterations = \", nIteration)\n",
        "        self.mIndicatorQ = tfArray.stack().numpy()\n",
        "\n",
        "    def estimate(self, mObservationG, Nproteins, Nk, psi):\n",
        "        \n",
        "        print('psi = ', psi)\n",
        "\n",
        "        mix_p = (1.0/float(Nk))*np.ones(Nk, dtype=float)\n",
        "        alpha1 = 1e-8\n",
        "        for i in range(Nproteins):\n",
        "            self.mIndicatorQ[i] = np.random.uniform(0.0, 1.0, size=Nk)\n",
        "            self.mIndicatorQ[i] = (self.mIndicatorQ[i] + alpha1)/(np.sum(self.mIndicatorQ[i]) + alpha1*Nproteins)\n",
        "\n",
        "        self.tf_annealing(mix_p, mObservationG, Nproteins, Nk, psi)\n",
        "        \n",
        "    def find_argmax(self):\n",
        "        self.indicatorVec = np.argmax(self.mIndicatorQ, axis=1)\n",
        "\n",
        "    def computeErrorRate(self, mObservationG, Nproteins):\n",
        "        \n",
        "        # self.find_lin_dependent()\n",
        "        self.find_argmax()\n",
        "\n",
        "        rnk = np.linalg.matrix_rank(self.mIndicatorQ)\n",
        "        print(\"Indicator matrix had rank = \" + str(rnk))\n",
        "        nClusters = len(np.unique(self.indicatorVec))\n",
        "        print(\"Number of clusters used: \" + str(nClusters))\n",
        "\n",
        "        countTp = 0\n",
        "        countTn = 0\n",
        "        countFn = 0\n",
        "        countFp = 0\n",
        "        sumSameCluster = 0\n",
        "        sumDiffCluster = 0\n",
        "        for i in range(Nproteins):\n",
        "            for j in mObservationG.lstAdjacency[i]:\n",
        "                t = mObservationG.mTrials[i][j]\n",
        "                s = mObservationG.mObserved[i][j]\n",
        "                assert(s <= t)\n",
        "                if (self.indicatorVec[i] == self.indicatorVec[j]):\n",
        "                    countTp += s\n",
        "                    countFn += (t - s)\n",
        "                    sumSameCluster += t\n",
        "                else:\n",
        "                    countTn += (t - s)\n",
        "                    countFp += s\n",
        "                    sumDiffCluster += t\n",
        "\n",
        "        counts = countFn + countFp\n",
        "        fn = 0.0\n",
        "        fp = 0.0\n",
        "        assert(sumSameCluster > 0)\n",
        "        fn = float(countFn)/float(sumSameCluster)\n",
        "        assert(sumDiffCluster > 0)\n",
        "        fp = float(countFp)/float(sumDiffCluster)\n",
        "        psi = (-np.log(fp) + np.log(1.0 - fn))/(-np.log(fn) + np.log(1.0 - fp))\n",
        "        likelihood = 0.0 \n",
        "        for i in np.arange(Nproteins):\n",
        "            for j in np.arange(i+1, Nproteins):\n",
        "                t = mObservationG.mTrials[i][j]\n",
        "                s = mObservationG.mObserved[i][j]\n",
        "                if (self.indicatorVec[i] == self.indicatorVec[j]):\n",
        "                  likelihood += (t-s)\n",
        "                else:    \n",
        "                  likelihood += s*psi\n",
        "        fdr = float(countFp)/float(countTp + countFp)\n",
        "        fdr2 = float(countFn)/float(countTn + countFn)\n",
        "        observed = np.array[[ countTp, countFp], [countFn, countTn]]\n",
        "        chi, p, dof, exptd = chi2_contingency(observed, lambda_=\"log-likelihood\")\n",
        "        return (fdr, fdr2, p, likelihood)\n",
        "        \n",
        "class CountMatrixModel:\n",
        "    \n",
        "    def __init__(self, nProteins, bait_inds, incidence):\n",
        "\n",
        "        self.nProteins = nProteins\n",
        "        self.mObserved = np.zeros(shape=(nProteins, nProteins), dtype=int)\n",
        "        for i, bait in zip(range(len(bait_inds)), bait_inds):\n",
        "            for j in range(nProteins):\n",
        "                if incidence[i,j]:\n",
        "                    self.mObserved[j,:] += incidence[i,:] \n",
        "                    self.mObserved[:,j] += incidence[i,:]\n",
        "    \n",
        "        self.mTrials = np.zeros(shape=(nProteins, nProteins), dtype=int)\n",
        "        for i, bait in zip(range(len(bait_inds)), bait_inds):\n",
        "            for j in range(nProteins):\n",
        "                if incidence[i,j]:\n",
        "                    self.mTrials[j,:] += np.ones(nProteins, dtype=int) \n",
        "                    self.mTrials[:,j] += np.ones(nProteins, dtype=int)\n",
        "\n",
        "        for i in range(nProteins):\n",
        "            assert(np.sum(self.mTrials[i,:]) == np.sum(self.mTrials[:,i]))\n",
        "\n",
        "        #\n",
        "        # Create the adjacency list\n",
        "        #\n",
        "        self.lstAdjacency = {}\n",
        "        for i in np.arange(nProteins):\n",
        "            self.lstAdjacency[i] = set()\n",
        "            for j in np.arange(nProteins):\n",
        "                t = self.mTrials[i][j]\n",
        "                if (i < j):\n",
        "                    s = self.mObserved[i][j] \n",
        "                else:\n",
        "                    s = self.mObserved[j][i] \n",
        "                assert(s <= t)\n",
        "                if (i != j and t > 0):\n",
        "                    self.lstAdjacency[i].add(j)\n",
        "#\n",
        "# TODO: cpmFunc can be countSpokeModel or countMatrixModel\n",
        "#\n",
        "class CInputSet:\n",
        "\n",
        "    def __init__(self, filename, cpmFunc=None):\n",
        "        super().__init__()\n",
        "\n",
        "        listBaits = list()\n",
        "        with open(filename) as fh:\n",
        "            setProteins = set()\n",
        "            for line in fh:\n",
        "                lst = line.rstrip().split(',')\n",
        "                bait = lst[0]\n",
        "                listBaits.append(bait)\n",
        "                setProteins = setProteins.union(set(lst))\n",
        "            print('Number of proteins ' + str(len(setProteins)))\n",
        "            fh.close()\n",
        "\n",
        "        self.aSortedProteins = np.sort(np.array(list(setProteins), dtype='U21'))\n",
        "        bait_inds = np.searchsorted(self.aSortedProteins, np.array(listBaits, dtype='U21'))\n",
        "        \n",
        "        print('Number of purifications ' + str(len(bait_inds)))\n",
        "\n",
        "        nProteins = len(self.aSortedProteins)\n",
        "        self.incidence = np.zeros(shape=(len(bait_inds), nProteins), dtype=int)\n",
        "        with open(filename) as fh:\n",
        "            lineCount = 0\n",
        "            for line in fh:\n",
        "                lst = line.rstrip().split(',')\n",
        "                prey_inds = np.searchsorted(self.aSortedProteins, np.array(lst, dtype='U21'))           \n",
        "                for id in prey_inds:\n",
        "                    self.incidence[lineCount][id] = 1\n",
        "                lineCount += 1\n",
        "            fh.close()\n",
        "            \n",
        "        self.observationG = CountMatrixModel(nProteins, bait_inds, self.incidence)\n",
        "\n",
        "    def writeCluster2File(self, baseName, matQ, indVec):\n",
        "        nRows, nCols = matQ.shape\n",
        "        filePath = baseName + \".tab\"\n",
        "        with open(filePath, \"w\") as fh:\n",
        "            for i in range(nRows):\n",
        "                ind = indVec[i]\n",
        "                fh.write(self.aSortedProteins[i] + '\\t' + str(indVec[i]) + '\\t' + str(max(matQ[ind])) + '\\n')\n",
        "            fh.close()\n",
        "        filePath = baseName + \".csv\"\n",
        "        with open(filePath, \"w\") as fh:\n",
        "            for k in range(nCols):\n",
        "                inds = list(i for i in range(nRows) if indVec[i] == k)\n",
        "                for j in inds:\n",
        "                    protein = self.aSortedProteins[j].split('__')[0] \n",
        "                    fh.write(protein + '\\t')\n",
        "                fh.write('\\n')\n",
        "            fh.close()\n",
        "    \n",
        "    def writeLabel2File(self, indVec):\n",
        "        clusters = {}\n",
        "        for i,k in enumerate(indVec):\n",
        "            if k not in clusters.keys():\n",
        "                clusters[k] = set()\n",
        "            clusters[k].add(i)\n",
        "\n",
        "        with open(\"out.csv\", \"w\") as fh:\n",
        "            for i, k in enumerate(clusters):\n",
        "                for v in clusters[k]:\n",
        "                    protein = self.aSortedProteins[v].split('__')[0] \n",
        "                    fh.write(protein + '\\t')\n",
        "                fh.write('\\n')\n",
        "            fh.close()\n",
        "\n",
        "def clustering(inputSet, Nk, psi, baseName):\n",
        "    fn = 0.8\n",
        "    fp = 0.04\n",
        "    nProteins = inputSet.observationG.nProteins\n",
        "    cmfa = CMeanFieldAnnealing(nProteins, Nk) # default\n",
        "\n",
        "    funcInfer = cmfa\n",
        "\n",
        "    ts = timer()\n",
        "    # alpha = 1e-2\n",
        "    funcInfer.estimate(inputSet.observationG, nProteins, Nk, psi) \n",
        "    te = timer()\n",
        "    print(\"Time running MFA: \", te-ts)\n",
        "    \n",
        "    funcInfer.find_argmax()\n",
        "    \n",
        "    inputSet.writeCluster2File(baseName, funcInfer.mIndicatorQ, funcInfer.indicatorVec)\n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVoono0R48_e",
        "outputId": "a108f0e1-735c-4ea0-bc12-64cb28ae19fb"
      },
      "outputs": [],
      "source": [
        "inputSet = CInputSet(\"gavin2006.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCHw7Cnq55bI",
        "outputId": "269f3da8-6e96-49be-c1c5-95d9c89f2a8c"
      },
      "outputs": [],
      "source": [
        "from time import time as timer\n",
        "\n",
        "def hill_climbing(inputSet, Nk, step=0.5):\n",
        "\n",
        "    nProteins = inputSet.observationG.nProteins\n",
        "    cmfa = CMeanFieldAnnealing(nProteins, Nk) # default\n",
        "\n",
        "    funcInfer = cmfa        \n",
        "\n",
        "    funcInfer.estimate(inputSet.observationG, nProteins, Nk, 0.3)\n",
        "    (fn, fp, p, f_last) = funcInfer.computeErrorRate(inputSet.observationG, nProteins)\n",
        "    x_values = np.arange(1.0, 10.5, step)\n",
        "    y_values = np.zeros(len(x_values), dtype=float)\n",
        "    p_values = np.zeros(len(x_values), dtype=float)\n",
        "    aics = np.zeros(len(x_values), dtype=float) \n",
        "    for i, psi in enumerate(x_values):\n",
        "        ts = timer()\n",
        "        f_value = funcInfer.estimate(inputSet.observationG, nProteins, Nk, psi) \n",
        "        te = timer()\n",
        "        print(\"Time running MFA: \", te-ts)\n",
        "        print(\"x = \", psi, \"f(x) = \", f_value)\n",
        "        (fdr, fn, p, likelihood) = funcInfer.computeErrorRate(inputSet.observationG, nProteins)\n",
        "        print(\"\\tP-value =\", p)\n",
        "        y_values[i] = likelihood\n",
        "        aics[i] = (Nk + likelihood)\n",
        "        p_values[i] = p\n",
        "        f_last = likelihood\n",
        "\n",
        "    return (x_values, p_values, likelihood)\n",
        "\n",
        "x_values, p_values, y_values = hill_climbing(inputSet, 300, step=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pdu5k-FR9rW0"
      },
      "outputs": [],
      "source": [
        "nProteins = inputSet.observationG.nProteins\n",
        "Nk = 300\n",
        "bics = np.log(nProteins)*Nk + 2.0*y_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBgqBxAAAL8i"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJKNm2ENAVVl"
      },
      "outputs": [],
      "source": [
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "print(np.log(0.99))\n",
        "ys = y_values\n",
        "y_filter = gaussian_filter1d(ys, 10)\n",
        "infls = np.where(ys <= 0.05*np.min(aics))[0]\n",
        "print(\"psi = \", x_values[infls])\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(x_values, ys, label='AIC')\n",
        "plt.legend(bbox_to_anchor=(1.5, 1.0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRuiiZ8SLsS2"
      },
      "outputs": [],
      "source": [
        "def clustering(inputSet, k, psi):\n",
        "    nProteins = inputSet.observationG.nProteins\n",
        "    cmfa = CMeanFieldAnnealing(nProteins, k) # default\n",
        "\n",
        "    funcInfer = cmfa        \n",
        "\n",
        "    funcInfer.estimate(inputSet.observationG, nProteins, k, psi)\n",
        "    (fn, fp, errs, f_last) = funcInfer.computeErrorRate(inputSet.observationG, nProteins)\n",
        "    return f_last\n",
        "    \n",
        "max_k = int(inputSet.observationG.nProteins/2)\n",
        "ks = np.arange(100, max_k, 100)\n",
        "for infl in infls[:1]:\n",
        "    ls = []\n",
        "    for i, k in enumerate(ks):\n",
        "        ls.append(clustering(inputSet, k, x_values[infl]))\n",
        "    bics = []\n",
        "    for i, f in enumerate(ls):\n",
        "        bics.append(np.log(nProteins)*float(ks[i]) + 2.0*ls[i])\n",
        "    plt.plot(ks, bics, label=\"{:.2f}\".format(x_values[infl]))\n",
        "plt.legend(bbox_to_anchor=(1.5, 1.0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXCekphfpjsI"
      },
      "outputs": [],
      "source": [
        "xs = ks[2:]\n",
        "ys = bics[2:]\n",
        "plt.plot(xs, np.min(ys)/ys)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCut2E_xCT4n"
      },
      "outputs": [],
      "source": [
        "plt.plot(ks[3:], bics[3:])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNQ+F2JQqWMMGPp+VtP7UqN",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1KG7qg50nrbipoLWTFmuQe5KbjthN3lmM",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit ('anaconda3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "96762580dc771c728ac9a1b8aa29a3a420bc09545a8c1a32553175fbb1f6eb2a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
