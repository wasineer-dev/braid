{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and read the input file\n",
    "\n",
    "We first import all necessary classes and read the input file. While reading the input file, we count the observation using the matrix model by supplying the class CountMatrixModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inputFile.inputFile as inputFile\n",
    "import spoke_model.countMatrixModel as cmm\n",
    "import meanfield.simulateLikelihood as smlt\n",
    "\n",
    "fileName = \"gavin2002.csv\"\n",
    "print('Hello, ' + fileName)\n",
    "inputSet = inputFile.CInputSet(fileName, cmm.CountMatrixModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter search\n",
    "\n",
    "To search for a good $\\psi$, we run a simple hill-climbing search until no further improvement is found.\n",
    "We compute both the negative log-likelihood and the AIC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time as timer\n",
    "\n",
    "def hill_climbing(inputSet, Nk, step=0.5):\n",
    "\n",
    "    nProteins = inputSet.observationG.nProteins\n",
    "    cmfa = smlt.CMeanFieldAnnealing(nProteins, Nk) # default\n",
    "\n",
    "    funcInfer = cmfa        \n",
    "\n",
    "    funcInfer.estimate(inputSet.observationG, nProteins, Nk, 0.3)\n",
    "    (fn, fp, errs, f_last) = funcInfer.computeErrorRate(inputSet.observationG, nProteins)\n",
    "    x_values = np.arange(1.0, 10.5, step)\n",
    "    y_values = np.zeros(len(x_values), dtype=float)\n",
    "    aics = np.zeros(len(x_values), dtype=float) \n",
    "    for i, psi in enumerate(x_values):\n",
    "        ts = timer()\n",
    "        f_value = funcInfer.estimate(inputSet.observationG, nProteins, Nk, psi) \n",
    "        te = timer()\n",
    "        print(\"Time running MFA: \", te-ts)\n",
    "        print(\"x = \", psi, \"f(x) = \", f_value)\n",
    "        (fn, fp, errs, likelihood) = funcInfer.computeErrorRate(inputSet.observationG, nProteins)\n",
    "        print(\"\\tLikelihood =\", likelihood)\n",
    "        y_values[i] = likelihood\n",
    "        aics[i] = (Nk - likelihood)/(Nk - f_last)\n",
    "        f_last = likelihood\n",
    "\n",
    "    return (x_values, aics, y_values)\n",
    "\n",
    "x_values, aics, y_values = hill_climbing(inputSet, 300, step=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the negative log-likelihood and the AIC\n",
    "\n",
    "We first filter the raw data, before we locate the inflection points. Here, the inflection points define values where the gradient change direction.\n",
    "Since $\\psi$ and the number of clusters are related, we will later fix $\\psi$ and run another search on a number of clusters.\n",
    "Smaller $\\psi$ results in splitting up clusters, while larger $\\psi$ tends to merge clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "aics_filter = gaussian_filter1d(aics, 1)\n",
    "y_values = y_values/np.max(y_values)\n",
    "y_filter = gaussian_filter1d(y_values, 1)\n",
    "d2 = np.gradient(np.gradient(y_filter))\n",
    "aics_d2 = np.gradient(np.gradient(aics_filter))\n",
    "infls = np.where(np.diff(np.sign(aics_d2)))[0]\n",
    "print(\"psi = \", x_values[infls])\n",
    "\n",
    "plt.plot(x_values, aics_filter, label='AIC Filter')\n",
    "plt.plot(x_values, aics, label='AIC')\n",
    "for i, infl in enumerate(infls):\n",
    "    plt.axvline(x=x_values[infl], color='k')\n",
    "plt.legend(bbox_to_anchor=(1.5, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of clusters\n",
    "\n",
    "In this example, we set $\\psi$ to be the first inflection points and search for a number of clusters by plotting the log-likelihood (scaled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(inputSet, k, psi):\n",
    "    nProteins = inputSet.observationG.nProteins\n",
    "    cmfa = smlt.CMeanFieldAnnealing(nProteins, k) # default\n",
    "\n",
    "    funcInfer = cmfa        \n",
    "\n",
    "    funcInfer.estimate(inputSet.observationG, nProteins, k, psi)\n",
    "    (fn, fp, errs, f_last) = funcInfer.computeErrorRate(inputSet.observationG, nProteins)\n",
    "    return f_last\n",
    "\n",
    "ks = np.arange(100, 700, 100)\n",
    "for infl in infls[:3]:\n",
    "    ls = []\n",
    "    for i, k in enumerate(ks):\n",
    "        ls.append(clustering(inputSet, k, x_values[infl]))\n",
    "    ls = ls/np.max(ls)\n",
    "    plt.plot(ks, ls)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96762580dc771c728ac9a1b8aa29a3a420bc09545a8c1a32553175fbb1f6eb2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
