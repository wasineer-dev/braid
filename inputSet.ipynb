{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1KG7qg50nrbipoLWTFmuQe5KbjthN3lmM",
      "authorship_tag": "ABX9TyNQ+F2JQqWMMGPp+VtP7UqN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wasineer-dev/braid/blob/develop/inputSet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "_Z-iHVgrLLWI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from time import time as timer\n",
        "\n",
        "MAX_ITERATION = 20\n",
        "\n",
        "class CMeanFieldAnnealing:\n",
        "\n",
        "    def __init__(self, Nproteins, Nk):\n",
        "        self.lstExpectedLikelihood = []\n",
        "        self.mIndicatorQ = np.zeros((Nproteins, Nk), dtype=float)\n",
        "\n",
        "    def tf_annealing(self, mix_p, mObservationG, Nproteins, Nk, psi):\n",
        "\n",
        "        matA = tf.convert_to_tensor(mObservationG.mTrials - mObservationG.mObserved, dtype=tf.float32)\n",
        "        matB = tf.convert_to_tensor(psi*mObservationG.mObserved, dtype=tf.float32)\n",
        "        tfArray = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "        for i in range(Nproteins):\n",
        "            tfArray = tfArray.write(i, self.mIndicatorQ[i])\n",
        "        \n",
        "        gamma = 1000.0\n",
        "        nIteration = 0\n",
        "        while(nIteration < MAX_ITERATION and gamma > 1.0):\n",
        "            for i in range(Nproteins):        \n",
        "                tQ = tfArray.stack()\n",
        "                fn_out = tf.tensordot(matA[i], tQ, axes=1) \n",
        "                fp_out = tf.tensordot(matB[i], 1.0 - tQ, axes=1)\n",
        "\n",
        "                mLogLikelihood = fn_out + fp_out\n",
        "                tfArray = tfArray.write(i, tf.nn.softmax(-gamma*mLogLikelihood))\n",
        "\n",
        "            nIteration += 1\n",
        "            gamma = gamma - 100.0\n",
        "        print(\"Initialize with MFA: num. iterations = \", nIteration)\n",
        "        self.mIndicatorQ = tfArray.stack().numpy()\n",
        "\n",
        "    def estimate(self, mObservationG, Nproteins, Nk, psi):\n",
        "        \n",
        "        print('psi = ', psi)\n",
        "\n",
        "        mix_p = (1.0/float(Nk))*np.ones(Nk, dtype=float)\n",
        "        alpha1 = 1e-8\n",
        "        for i in range(Nproteins):\n",
        "            self.mIndicatorQ[i] = np.random.uniform(0.0, 1.0, size=Nk)\n",
        "            self.mIndicatorQ[i] = (self.mIndicatorQ[i] + alpha1)/(np.sum(self.mIndicatorQ[i]) + alpha1*Nproteins)\n",
        "\n",
        "        self.tf_annealing(mix_p, mObservationG, Nproteins, Nk, psi)\n",
        "        \n",
        "    def find_argmax(self):\n",
        "        self.indicatorVec = np.argmax(self.mIndicatorQ, axis=1)\n",
        "\n",
        "    def computeErrorRate(self, mObservationG, Nproteins):\n",
        "        \n",
        "        # self.find_lin_dependent()\n",
        "        self.find_argmax()\n",
        "\n",
        "        rnk = np.linalg.matrix_rank(self.mIndicatorQ)\n",
        "        print(\"Indicator matrix had rank = \" + str(rnk))\n",
        "        nClusters = len(np.unique(self.indicatorVec))\n",
        "        print(\"Number of clusters used: \" + str(nClusters))\n",
        "\n",
        "        countFn = 0\n",
        "        countFp = 0\n",
        "        sumSameCluster = 0\n",
        "        sumDiffCluster = 0\n",
        "        for i in range(Nproteins):\n",
        "            for j in mObservationG.lstAdjacency[i]:\n",
        "                t = mObservationG.mTrials[i][j]\n",
        "                s = mObservationG.mObserved[i][j]\n",
        "                assert(s <= t)\n",
        "                if (self.indicatorVec[i] == self.indicatorVec[j]):\n",
        "                    countFn += (t - s)\n",
        "                    sumSameCluster += t\n",
        "                else:\n",
        "                    countFp += s\n",
        "                    sumDiffCluster += t\n",
        "\n",
        "        counts = countFn + countFp\n",
        "        fn = 0.0\n",
        "        fp = 0.0\n",
        "        if (sumSameCluster > 0):\n",
        "            fn = float(countFn)/float(sumSameCluster)\n",
        "        if (sumDiffCluster > 0):\n",
        "            fp = float(countFp)/float(sumDiffCluster)\n",
        "        likelihood = countFn*(-np.log(fn)) + countFp*(-np.log(fp)) \n",
        "        for i in range(Nproteins):\n",
        "            for j in mObservationG.lstAdjacency[i]:\n",
        "                t = mObservationG.mTrials[i][j]\n",
        "                s = mObservationG.mObserved[i][j]\n",
        "                likelihood += -s*(np.log(1.0-fn)) - (t-s)*(np.log(1.0-fp))\n",
        "        return (fn, fp, counts, likelihood)\n",
        "        \n",
        "class CountMatrixModel:\n",
        "    \n",
        "    def __init__(self, nProteins, bait_inds, incidence):\n",
        "\n",
        "        self.nProteins = nProteins\n",
        "        self.mObserved = np.zeros(shape=(nProteins, nProteins), dtype=int)\n",
        "        for i, bait in zip(range(len(bait_inds)), bait_inds):\n",
        "            for j in range(nProteins):\n",
        "                if incidence[i,j]:\n",
        "                    self.mObserved[j,:] += incidence[i,:] \n",
        "                    self.mObserved[:,j] += incidence[i,:]\n",
        "    \n",
        "        self.mTrials = np.zeros(shape=(nProteins, nProteins), dtype=int)\n",
        "        for i, bait in zip(range(len(bait_inds)), bait_inds):\n",
        "            for j in range(nProteins):\n",
        "                if incidence[i,j]:\n",
        "                    self.mTrials[j,:] += np.ones(nProteins, dtype=int) \n",
        "                    self.mTrials[:,j] += np.ones(nProteins, dtype=int)\n",
        "\n",
        "        for i in range(nProteins):\n",
        "            assert(np.sum(self.mTrials[i,:]) == np.sum(self.mTrials[:,i]))\n",
        "\n",
        "        #\n",
        "        # Create the adjacency list\n",
        "        #\n",
        "        self.lstAdjacency = {}\n",
        "        for i in np.arange(nProteins):\n",
        "            self.lstAdjacency[i] = set()\n",
        "            for j in np.arange(nProteins):\n",
        "                t = self.mTrials[i][j]\n",
        "                if (i < j):\n",
        "                    s = self.mObserved[i][j] \n",
        "                else:\n",
        "                    s = self.mObserved[j][i] \n",
        "                assert(s <= t)\n",
        "                if (i != j and t > 0):\n",
        "                    self.lstAdjacency[i].add(j)\n",
        "#\n",
        "# TODO: cpmFunc can be countSpokeModel or countMatrixModel\n",
        "#\n",
        "class CInputSet:\n",
        "\n",
        "    def __init__(self, filename, cpmFunc=None):\n",
        "        super().__init__()\n",
        "\n",
        "        listBaits = list()\n",
        "        with open(filename) as fh:\n",
        "            setProteins = set()\n",
        "            for line in fh:\n",
        "                lst = line.rstrip().split(',')\n",
        "                bait = lst[0]\n",
        "                listBaits.append(bait)\n",
        "                setProteins = setProteins.union(set(lst))\n",
        "            print('Number of proteins ' + str(len(setProteins)))\n",
        "            fh.close()\n",
        "\n",
        "        self.aSortedProteins = np.sort(np.array(list(setProteins), dtype='U21'))\n",
        "        bait_inds = np.searchsorted(self.aSortedProteins, np.array(listBaits, dtype='U21'))\n",
        "        \n",
        "        print('Number of purifications ' + str(len(bait_inds)))\n",
        "\n",
        "        nProteins = len(self.aSortedProteins)\n",
        "        self.incidence = np.zeros(shape=(len(bait_inds), nProteins), dtype=int)\n",
        "        with open(filename) as fh:\n",
        "            lineCount = 0\n",
        "            for line in fh:\n",
        "                lst = line.rstrip().split(',')\n",
        "                prey_inds = np.searchsorted(self.aSortedProteins, np.array(lst, dtype='U21'))           \n",
        "                for id in prey_inds:\n",
        "                    self.incidence[lineCount][id] = 1\n",
        "                lineCount += 1\n",
        "            fh.close()\n",
        "            \n",
        "        self.observationG = CountMatrixModel(nProteins, bait_inds, self.incidence)\n",
        "\n",
        "    def writeCluster2File(self, baseName, matQ, indVec):\n",
        "        nRows, nCols = matQ.shape\n",
        "        filePath = baseName + \".tab\"\n",
        "        with open(filePath, \"w\") as fh:\n",
        "            for i in range(nRows):\n",
        "                ind = indVec[i]\n",
        "                fh.write(self.aSortedProteins[i] + '\\t' + str(indVec[i]) + '\\t' + str(max(matQ[ind])) + '\\n')\n",
        "            fh.close()\n",
        "        filePath = baseName + \".csv\"\n",
        "        with open(filePath, \"w\") as fh:\n",
        "            for k in range(nCols):\n",
        "                inds = list(i for i in range(nRows) if indVec[i] == k)\n",
        "                for j in inds:\n",
        "                    protein = self.aSortedProteins[j].split('__')[0] \n",
        "                    fh.write(protein + '\\t')\n",
        "                fh.write('\\n')\n",
        "            fh.close()\n",
        "    \n",
        "    def writeLabel2File(self, indVec):\n",
        "        clusters = {}\n",
        "        for i,k in enumerate(indVec):\n",
        "            if k not in clusters.keys():\n",
        "                clusters[k] = set()\n",
        "            clusters[k].add(i)\n",
        "\n",
        "        with open(\"out.csv\", \"w\") as fh:\n",
        "            for i, k in enumerate(clusters):\n",
        "                for v in clusters[k]:\n",
        "                    protein = self.aSortedProteins[v].split('__')[0] \n",
        "                    fh.write(protein + '\\t')\n",
        "                fh.write('\\n')\n",
        "            fh.close()\n",
        "\n",
        "def clustering(inputSet, Nk, psi, baseName):\n",
        "    fn = 0.8\n",
        "    fp = 0.04\n",
        "    nProteins = inputSet.observationG.nProteins\n",
        "    cmfa = CMeanFieldAnnealing(nProteins, Nk) # default\n",
        "\n",
        "    funcInfer = cmfa\n",
        "\n",
        "    ts = timer()\n",
        "    # alpha = 1e-2\n",
        "    funcInfer.estimate(inputSet.observationG, nProteins, Nk, psi) \n",
        "    te = timer()\n",
        "    print(\"Time running MFA: \", te-ts)\n",
        "    \n",
        "    funcInfer.find_argmax()\n",
        "    \n",
        "    inputSet.writeCluster2File(baseName, funcInfer.mIndicatorQ, funcInfer.indicatorVec)\n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputSet = CInputSet(\"/content/drive/MyDrive/gavin2006.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVoono0R48_e",
        "outputId": "a108f0e1-735c-4ea0-bc12-64cb28ae19fb"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of proteins 2760\n",
            "Number of purifications 2166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time as timer\n",
        "\n",
        "def hill_climbing(inputSet, Nk, step=0.5):\n",
        "\n",
        "    nProteins = inputSet.observationG.nProteins\n",
        "    cmfa = CMeanFieldAnnealing(nProteins, Nk) # default\n",
        "\n",
        "    funcInfer = cmfa        \n",
        "\n",
        "    funcInfer.estimate(inputSet.observationG, nProteins, Nk, 0.3)\n",
        "    (fn, fp, errs, f_last) = funcInfer.computeErrorRate(inputSet.observationG, nProteins)\n",
        "    x_values = np.arange(1.0, 10.5, step)\n",
        "    y_values = np.zeros(len(x_values), dtype=float)\n",
        "    aics = np.zeros(len(x_values), dtype=float) \n",
        "    for i, psi in enumerate(x_values):\n",
        "        ts = timer()\n",
        "        f_value = funcInfer.estimate(inputSet.observationG, nProteins, Nk, psi) \n",
        "        te = timer()\n",
        "        print(\"Time running MFA: \", te-ts)\n",
        "        print(\"x = \", psi, \"f(x) = \", f_value)\n",
        "        (fn, fp, errs, likelihood) = funcInfer.computeErrorRate(inputSet.observationG, nProteins)\n",
        "        print(\"\\tLikelihood =\", likelihood)\n",
        "        y_values[i] = likelihood\n",
        "        aics[i] = (Nk + likelihood)\n",
        "        f_last = likelihood\n",
        "\n",
        "    return (x_values, aics, y_values)\n",
        "\n",
        "x_values, aics, y_values = hill_climbing(inputSet, 300, step=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCHw7Cnq55bI",
        "outputId": "269f3da8-6e96-49be-c1c5-95d9c89f2a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "psi =  0.3\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Indicator matrix had rank = 300\n",
            "Number of clusters used: 300\n",
            "psi =  1.0\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  106.10355043411255\n",
            "x =  1.0 f(x) =  None\n",
            "Indicator matrix had rank = 300\n",
            "Number of clusters used: 300\n",
            "\tLikelihood = 5748785.574634841\n",
            "psi =  1.5\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  107.31715846061707\n",
            "x =  1.5 f(x) =  None\n",
            "Indicator matrix had rank = 300\n",
            "Number of clusters used: 300\n",
            "\tLikelihood = 5614516.030721573\n",
            "psi =  2.0\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  106.86240315437317\n",
            "x =  2.0 f(x) =  None\n",
            "Indicator matrix had rank = 300\n",
            "Number of clusters used: 300\n",
            "\tLikelihood = 5544625.152860571\n",
            "psi =  2.5\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  106.9421136379242\n",
            "x =  2.5 f(x) =  None\n",
            "Indicator matrix had rank = 300\n",
            "Number of clusters used: 300\n",
            "\tLikelihood = 5494283.476093733\n",
            "psi =  3.0\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  106.92888522148132\n",
            "x =  3.0 f(x) =  None\n",
            "Indicator matrix had rank = 300\n",
            "Number of clusters used: 300\n",
            "\tLikelihood = 5472332.530500542\n",
            "psi =  3.5\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  106.18069744110107\n",
            "x =  3.5 f(x) =  None\n",
            "Indicator matrix had rank = 300\n",
            "Number of clusters used: 300\n",
            "\tLikelihood = 5451408.484056204\n",
            "psi =  4.0\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  106.10795187950134\n",
            "x =  4.0 f(x) =  None\n",
            "Indicator matrix had rank = 300\n",
            "Number of clusters used: 300\n",
            "\tLikelihood = 5437041.34634061\n",
            "psi =  4.5\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  107.0244529247284\n",
            "x =  4.5 f(x) =  None\n",
            "Indicator matrix had rank = 300\n",
            "Number of clusters used: 300\n",
            "\tLikelihood = 5433664.784302675\n",
            "psi =  5.0\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  107.56606841087341\n",
            "x =  5.0 f(x) =  None\n",
            "Indicator matrix had rank = 300\n",
            "Number of clusters used: 300\n",
            "\tLikelihood = 5446885.678415698\n",
            "psi =  5.5\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  107.09229230880737\n",
            "x =  5.5 f(x) =  None\n",
            "Indicator matrix had rank = 300\n",
            "Number of clusters used: 300\n",
            "\tLikelihood = 5427500.691043609\n",
            "psi =  6.0\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  106.85999345779419\n",
            "x =  6.0 f(x) =  None\n",
            "Indicator matrix had rank = 300\n",
            "Number of clusters used: 300\n",
            "\tLikelihood = 5426578.400746896\n",
            "psi =  6.5\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  106.53950071334839\n",
            "x =  6.5 f(x) =  None\n",
            "Indicator matrix had rank = 300\n",
            "Number of clusters used: 300\n",
            "\tLikelihood = 5435278.349139958\n",
            "psi =  7.0\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  105.78230118751526\n",
            "x =  7.0 f(x) =  None\n",
            "Indicator matrix had rank = 300\n",
            "Number of clusters used: 300\n",
            "\tLikelihood = 5419535.285447477\n",
            "psi =  7.5\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  105.65435886383057\n",
            "x =  7.5 f(x) =  None\n",
            "Indicator matrix had rank = 300\n",
            "Number of clusters used: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nProteins = inputSet.observationG.nProteins\n",
        "Nk = 300\n",
        "bics = np.log(nProteins)*Nk + 2.0*y_values"
      ],
      "metadata": {
        "id": "Pdu5k-FR9rW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "TBgqBxAAAL8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "y_values = np.exp(np.min(y_values) - y_values)\n",
        "y_filter = gaussian_filter1d(y_values, 2)\n",
        "d2 = np.gradient(np.gradient(y_filter))\n",
        "aics_d2 = np.gradient(np.gradient(aics))\n",
        "infls = np.where(np.diff(np.sign(d2)))[0]\n",
        "print(\"psi = \", x_values[infls])\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(x_values, y_values, label='AIC')\n",
        "plt.plot(x_values, y_filter, label='AIC Filter')\n",
        "for i, infl in enumerate(infls):\n",
        "    plt.axvline(x=x_values[infl], color='k')\n",
        "plt.legend(bbox_to_anchor=(1.5, 1.0))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kJKNm2ENAVVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clustering(inputSet, k, psi):\n",
        "    nProteins = inputSet.observationG.nProteins\n",
        "    cmfa = CMeanFieldAnnealing(nProteins, k) # default\n",
        "\n",
        "    funcInfer = cmfa        \n",
        "\n",
        "    funcInfer.estimate(inputSet.observationG, nProteins, k, psi)\n",
        "    (fn, fp, errs, f_last) = funcInfer.computeErrorRate(inputSet.observationG, nProteins)\n",
        "    return f_last\n",
        "    \n",
        "max_k = int(inputSet.observationG.nProteins/2)\n",
        "ks = np.arange(100, max_k, 100)\n",
        "for infl in infls[:1]:\n",
        "    ls = []\n",
        "    for i, k in enumerate(ks):\n",
        "        ls.append(clustering(inputSet, k, x_values[infl]))\n",
        "    bics = []\n",
        "    for i, f in enumerate(ls):\n",
        "        bics.append(np.log(nProteins)*float(ks[i]) + 2.0*ls[i])\n",
        "    plt.plot(ks, bics, label=\"{:.2f}\".format(x_values[infl]))\n",
        "plt.legend(bbox_to_anchor=(1.5, 1.0))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TRuiiZ8SLsS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs = ks[2:]\n",
        "ys = bics[2:]\n",
        "plt.plot(xs, np.min(ys)/ys)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uXCekphfpjsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(ks[3:], bics[3:])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pCut2E_xCT4n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}