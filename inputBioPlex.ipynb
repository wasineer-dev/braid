{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1eJ5ssrsKFPCUP4XtOXXJjWvOb2rV-de1",
      "authorship_tag": "ABX9TyOva0QN7CWlK52kae+Z+pQ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wasineer-dev/braid/blob/develop/inputBioPlex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bioplex.hms.harvard.edu/data/BaitPreyPairs_noFilters_BP2a.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-uYByuKot6C",
        "outputId": "5e7e86ce-5288-4964-ed3b-c04ebe4e6496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-19 01:54:13--  https://bioplex.hms.harvard.edu/data/BaitPreyPairs_noFilters_BP2a.tsv\n",
            "Resolving bioplex.hms.harvard.edu (bioplex.hms.harvard.edu)... 45.60.80.9\n",
            "Connecting to bioplex.hms.harvard.edu (bioplex.hms.harvard.edu)|45.60.80.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 621906318 (593M) [text/tab-separated-values]\n",
            "Saving to: ‘BaitPreyPairs_noFilters_BP2a.tsv’\n",
            "\n",
            "BaitPreyPairs_noFil 100%[===================>] 593.10M  35.8MB/s    in 17s     \n",
            "\n",
            "2023-01-19 01:54:31 (34.4 MB/s) - ‘BaitPreyPairs_noFilters_BP2a.tsv’ saved [621906318/621906318]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf braid\n",
        "!git clone https://github.com/wasineer-dev/braid.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18s_8FrCe8xD",
        "outputId": "3af7910f-684b-4fd0-c73b-67487d86a0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'braid'...\n",
            "remote: Enumerating objects: 819, done.\u001b[K\n",
            "remote: Counting objects: 100% (183/183), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 819 (delta 102), reused 122 (delta 58), pack-reused 636\u001b[K\n",
            "Receiving objects: 100% (819/819), 13.17 MiB | 18.38 MiB/s, done.\n",
            "Resolving deltas: 100% (462/462), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/braid\n",
        "!git switch develop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtW1_TPeqyqt",
        "outputId": "8251e192-7e79-4498-fa4f-586d62c7126e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/braid\n",
            "Branch 'develop' set up to track remote branch 'develop' from 'origin'.\n",
            "Switched to a new branch 'develop'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "path = f\"/content/braid\"\n",
        "if not path in sys.path:\n",
        "    sys.path.insert(1, path)\n",
        "\n",
        "# list all directories in the Python path\n",
        "print(\"\\n\".join([\"'\" + path + \"'\" for path in sys.path]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GodvW6dXfGPu",
        "outputId": "57a01d2f-1eb1-4a8c-b244-ccd6b977e2c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content'\n",
            "'/content/braid'\n",
            "'/env/python'\n",
            "'/usr/lib/python38.zip'\n",
            "'/usr/lib/python3.8'\n",
            "'/usr/lib/python3.8/lib-dynload'\n",
            "''\n",
            "'/usr/local/lib/python3.8/dist-packages'\n",
            "'/usr/lib/python3/dist-packages'\n",
            "'/usr/local/lib/python3.8/dist-packages/IPython/extensions'\n",
            "'/root/.ipython'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-r9jfe8z7Yw",
        "outputId": "d5134c6a-a1f1-41a2-fdcc-df6a646d0596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of baits =  5891\n",
            "Number of preys =  12150\n",
            "Number of proteins =  12260\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "from time import time as timer\n",
        "\n",
        "import importlib\n",
        "mymodule = importlib.import_module(\"spoke_model\")\n",
        "\n",
        "import importlib.util\n",
        "import sys\n",
        "spec = importlib.util.spec_from_file_location(\"cpmBioplex\", \"/content/braid/spoke_model/countBioplexSpokeModel.py\")\n",
        "cbSpokeModel = importlib.util.module_from_spec(spec)\n",
        "sys.modules[\"cpmBioplex\"] = cbSpokeModel\n",
        "spec.loader.exec_module(cbSpokeModel)\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(\"smlt\", \"/content/braid/meanfield/simulateLikelihood.py\")\n",
        "smlt = importlib.util.module_from_spec(spec)\n",
        "sys.modules[\"smlt\"] = smlt\n",
        "spec.loader.exec_module(smlt)\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(\"inputBioplex\", \"/content/braid/inputFile/inputBioplex.py\")\n",
        "inputBioplex = importlib.util.module_from_spec(spec)\n",
        "sys.modules[\"inputBioplex\"] = inputBioplex\n",
        "spec.loader.exec_module(inputBioplex)\n",
        "            \n",
        "def clustering(inputSet, Nk, psi, fileName):\n",
        "    nProteins = inputSet.observationG.nProteins\n",
        "    cmfa = smlt.CMeanFieldAnnealing(nProteins, Nk) # default\n",
        "\n",
        "    funcInfer = cmfa\n",
        "\n",
        "    ts = timer()\n",
        "    # alpha = 1e-2\n",
        "    funcInfer.estimate(inputSet.observationG, nProteins, Nk, psi) \n",
        "    te = timer()\n",
        "    print(\"Time running MFA: \", te-ts)\n",
        "\n",
        "    funcInfer.find_argmax()\n",
        "    inputSet.writeCluster2File(fileName, funcInfer.mIndicatorQ, funcInfer.indicatorVec)\n",
        "\n",
        "bioPlex2 = inputBioplex.CInputBioplex(\"/content/BaitPreyPairs_noFilters_BP2a.tsv\", cbSpokeModel.CountBioplexSpoke)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "clustering(bioPlex2, 2000, psi=2.5, fileName=\"bioplex_2.5_2000.tsv\")\n",
        "clustering(bioPlex2, 3000, psi=2.5, fileName=\"bioplex_2.5_3000.tsv\")\n",
        "clustering(bioPlex2, 4000, psi=2.5, fileName=\"bioplex_2.5_4000.tsv\")\n",
        "clustering(bioPlex2, 5000, psi=2.5, fileName=\"bioplex_2.5_5000.tsv\")\n"
      ],
      "metadata": {
        "id": "wLUArVyj7RG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331279d4-994f-4af0-bfcc-a7043ed8bf9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "psi =  2.5\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  1125.5291023254395\n",
            "psi =  2.5\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  1118.7022399902344\n",
            "psi =  2.5\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  1125.3675329685211\n",
            "psi =  2.5\n",
            "Initialize with MFA: num. iterations =  10\n",
            "Time running MFA:  1131.0644264221191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_ITERATION = 20\n",
        "\n",
        "class CMeanFieldAnnealing:\n",
        "\n",
        "    def __init__(self, Nproteins, Nk):\n",
        "        self.lstExpectedLikelihood = []\n",
        "        self.mIndicatorQ = np.zeros((Nproteins, Nk), dtype=float)\n",
        "\n",
        "    def tf_annealing(self, mix_p, mObservationG, Nproteins, Nk, psi):\n",
        "\n",
        "        matA = tf.convert_to_tensor(mObservationG.mTrials - mObservationG.mObserved, dtype=tf.float32)\n",
        "        matB = tf.convert_to_tensor(psi*mObservationG.mObserved, dtype=tf.float32)\n",
        "        tfArray = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "        for i in range(Nproteins):\n",
        "            tfArray = tfArray.write(i, self.mIndicatorQ[i])\n",
        "        \n",
        "        gamma = 1000.0\n",
        "        nIteration = 0\n",
        "        while(nIteration < MAX_ITERATION and gamma > 1.0):\n",
        "            for i in range(Nproteins):        \n",
        "                tQ = tfArray.stack()\n",
        "                fn_out = tf.tensordot(matA[i], tQ, axes=1) \n",
        "                fp_out = tf.tensordot(matB[i], 1.0 - tQ, axes=1)\n",
        "\n",
        "                mLogLikelihood = fn_out + fp_out\n",
        "                tfArray = tfArray.write(i, tf.nn.softmax(-gamma*mLogLikelihood))\n",
        "\n",
        "            nIteration += 1\n",
        "            gamma = gamma - 100.0\n",
        "        print(\"Initialize with MFA: num. iterations = \", nIteration)\n",
        "        self.mIndicatorQ = tfArray.stack().numpy()\n",
        "\n",
        "    def estimate(self, mObservationG, Nproteins, Nk, psi):\n",
        "        \n",
        "        print('psi = ', psi)\n",
        "\n",
        "        mix_p = (1.0/float(Nk))*np.ones(Nk, dtype=float)\n",
        "        alpha1 = 1e-8\n",
        "        for i in range(Nproteins):\n",
        "            self.mIndicatorQ[i] = np.random.uniform(0.0, 1.0, size=Nk)\n",
        "            self.mIndicatorQ[i] = (self.mIndicatorQ[i] + alpha1)/(np.sum(self.mIndicatorQ[i]) + alpha1*Nproteins)\n",
        "\n",
        "        with tf.device('/device:GPU:0'):\n",
        "          self.tf_annealing(mix_p, mObservationG, Nproteins, Nk, psi)\n",
        "\n",
        "    def find_argmax(self):\n",
        "        N = np.size(self.mIndicatorQ, axis=0)\n",
        "        k = np.size(self.mIndicatorQ, axis=1)\n",
        "        self.indicatorVec = np.argmax(self.mIndicatorQ, axis=1)\n",
        "        \n",
        "    def computeErrorRate(self, mObservationG, Nproteins):\n",
        "        \n",
        "        self.find_argmax()\n",
        "\n",
        "        nClusters = len(np.unique(self.indicatorVec))\n",
        "        print(\"Number of clusters used: \" + str(nClusters))\n",
        "\n",
        "        countFn = 0\n",
        "        countFp = 0\n",
        "        sumSameCluster = 0\n",
        "        sumDiffCluster = 0\n",
        "        for i in range(Nproteins):\n",
        "            for j in mObservationG.lstAdjacency[i]:\n",
        "                t = mObservationG.mTrials[i][j]\n",
        "                s = mObservationG.mObserved[i][j]\n",
        "                assert(s <= t)\n",
        "                if (self.indicatorVec[i] == self.indicatorVec[j]):\n",
        "                    countFn += (t - s)\n",
        "                    sumSameCluster += t\n",
        "                else:\n",
        "                    countFp += s\n",
        "                    sumDiffCluster += t\n",
        "\n",
        "        counts = countFn + countFp\n",
        "        fn = 0.0\n",
        "        fp = 0.0\n",
        "        if (sumSameCluster > 0):\n",
        "            fn = float(countFn)/float(sumSameCluster)\n",
        "        if (sumDiffCluster > 0):\n",
        "            fp = float(countFp)/float(sumDiffCluster)\n",
        "        likelihood = countFn*(-np.log(fn) + np.log(1.0 - fp)) + countFp*(-np.log(fp) + np.log(1.0 - fn)) \n",
        "        for i in range(Nproteins):\n",
        "            for j in mObservationG.lstAdjacency[i]:\n",
        "                t = mObservationG.mTrials[i][j]\n",
        "                s = mObservationG.mObserved[i][j]\n",
        "                likelihood += -s*(np.log(1.0-fn)) - (t-s)*(np.log(1.0-fp))\n",
        "        return (fn, fp, counts, likelihood)"
      ],
      "metadata": {
        "id": "_1UxRxR-4C0x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}