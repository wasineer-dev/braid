{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wasineer-dev/braid/blob/develop/keras_tuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Z-iHVgrLLWI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from time import time as timer\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "MAX_ITERATION = 20\n",
        "\n",
        "class CMeanFieldAnnealing:\n",
        "\n",
        "    def __init__(self, Nproteins, Nk):\n",
        "        self.lstExpectedLikelihood = []\n",
        "        self.mIndicatorQ = np.zeros((Nproteins, Nk), dtype=float)\n",
        "\n",
        "    def tf_annealing(self, mix_p, mObservationG, Nproteins, Nk, psi):\n",
        "\n",
        "        matA = tf.convert_to_tensor(mObservationG.mTrials - mObservationG.mObserved, dtype=tf.float32)\n",
        "        matB = tf.convert_to_tensor(psi*mObservationG.mObserved, dtype=tf.float32)\n",
        "        tfArray = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "        for i in range(Nproteins):\n",
        "            tfArray = tfArray.write(i, self.mIndicatorQ[i])\n",
        "        \n",
        "        gamma = 1000.0\n",
        "        nIteration = 0\n",
        "        while(nIteration < MAX_ITERATION and gamma > 1.0):\n",
        "            for i in range(Nproteins):        \n",
        "                tQ = tfArray.stack()\n",
        "                fn_out = tf.tensordot(matA[i], tQ, axes=1) \n",
        "                fp_out = tf.tensordot(matB[i], 1.0 - tQ, axes=1)\n",
        "\n",
        "                mLogLikelihood = fn_out + fp_out\n",
        "                tfArray = tfArray.write(i, tf.nn.softmax(-gamma*mLogLikelihood))\n",
        "\n",
        "            nIteration += 1\n",
        "            gamma = gamma - 100.0\n",
        "        print(\"Initialize with MFA: num. iterations = \", nIteration)\n",
        "        self.mIndicatorQ = tfArray.stack().numpy()\n",
        "\n",
        "    def estimate(self, mObservationG, Nproteins, Nk, psi):\n",
        "        \n",
        "        print('psi = ', psi)\n",
        "\n",
        "        mix_p = (1.0/float(Nk))*np.ones(Nk, dtype=float)\n",
        "        alpha1 = 1e-8\n",
        "        for i in range(Nproteins):\n",
        "            self.mIndicatorQ[i] = np.random.uniform(0.0, 1.0, size=Nk)\n",
        "            self.mIndicatorQ[i] = (self.mIndicatorQ[i] + alpha1)/(np.sum(self.mIndicatorQ[i]) + alpha1*Nproteins)\n",
        "\n",
        "        self.tf_annealing(mix_p, mObservationG, Nproteins, Nk, psi)\n",
        "        \n",
        "    def find_argmax(self):\n",
        "        self.indicatorVec = np.argmax(self.mIndicatorQ, axis=1)\n",
        "\n",
        "    def computeErrorRate(self, mObservationG, Nproteins):\n",
        "        \n",
        "        # self.find_lin_dependent()\n",
        "        self.find_argmax()\n",
        "\n",
        "        rnk = np.linalg.matrix_rank(self.mIndicatorQ)\n",
        "        print(\"Indicator matrix had rank = \" + str(rnk))\n",
        "        nClusters = len(np.unique(self.indicatorVec))\n",
        "        print(\"Number of clusters used: \" + str(nClusters))\n",
        "\n",
        "        countTp = 0\n",
        "        countTn = 0\n",
        "        countFn = 0\n",
        "        countFp = 0\n",
        "        sumSameCluster = 0\n",
        "        sumDiffCluster = 0\n",
        "        for i in range(Nproteins):\n",
        "            for j in mObservationG.lstAdjacency[i]:\n",
        "                t = mObservationG.mTrials[i][j]\n",
        "                s = mObservationG.mObserved[i][j]\n",
        "                assert(s <= t)\n",
        "                if (self.indicatorVec[i] == self.indicatorVec[j]):\n",
        "                    countTp += s\n",
        "                    countFn += (t - s)\n",
        "                    sumSameCluster += t\n",
        "                else:\n",
        "                    countTn += (t - s)\n",
        "                    countFp += s\n",
        "                    sumDiffCluster += t\n",
        "\n",
        "        counts = countFn + countFp\n",
        "        fn = 0.0\n",
        "        fp = 0.0\n",
        "        assert(sumSameCluster > 0)\n",
        "        fn = float(countFn)/float(sumSameCluster)\n",
        "        assert(sumDiffCluster > 0)\n",
        "        fp = float(countFp)/float(sumDiffCluster)\n",
        "        psi = (-np.log(fp) + np.log(1.0 - fn))/(-np.log(fn) + np.log(1.0 - fp))\n",
        "        likelihood = 0.0 \n",
        "        for i in np.arange(Nproteins):\n",
        "            for j in np.arange(i+1, Nproteins):\n",
        "                t = mObservationG.mTrials[i][j]\n",
        "                s = mObservationG.mObserved[i][j]\n",
        "                if (self.indicatorVec[i] == self.indicatorVec[j]):\n",
        "                  likelihood += (t-s)\n",
        "                else:    \n",
        "                  likelihood += s*psi\n",
        "\n",
        "        fdr = float(countFp)/float(countTp + countFp)\n",
        "        fdr2 = float(countFn)/float(countTn + countFn)\n",
        "        precision = float(countTp)/float(countTp + countFp)\n",
        "        recall = float(countTp)/float(countTp + countFn)\n",
        "        f1 = 2*(precision * recall)/(precision + recall)\n",
        "        return (precision, recall, f1, likelihood)\n",
        "        \n",
        "class CountMatrixModel:\n",
        "    \n",
        "    def __init__(self, nProteins, bait_inds, incidence):\n",
        "\n",
        "        self.nProteins = nProteins\n",
        "        self.mObserved = np.zeros(shape=(nProteins, nProteins), dtype=int)\n",
        "        for i, bait in zip(range(len(bait_inds)), bait_inds):\n",
        "            for j in range(nProteins):\n",
        "                if incidence[i,j]:\n",
        "                    self.mObserved[j,:] += incidence[i,:] \n",
        "                    self.mObserved[:,j] += incidence[i,:]\n",
        "    \n",
        "        self.mTrials = np.zeros(shape=(nProteins, nProteins), dtype=int)\n",
        "        for i, bait in zip(range(len(bait_inds)), bait_inds):\n",
        "            for j in range(nProteins):\n",
        "                if incidence[i,j]:\n",
        "                    self.mTrials[j,:] += np.ones(nProteins, dtype=int) \n",
        "                    self.mTrials[:,j] += np.ones(nProteins, dtype=int)\n",
        "\n",
        "        for i in range(nProteins):\n",
        "            assert(np.sum(self.mTrials[i,:]) == np.sum(self.mTrials[:,i]))\n",
        "\n",
        "        #\n",
        "        # Create the adjacency list\n",
        "        #\n",
        "        self.lstAdjacency = {}\n",
        "        for i in np.arange(nProteins):\n",
        "            self.lstAdjacency[i] = set()\n",
        "            for j in np.arange(nProteins):\n",
        "                t = self.mTrials[i][j]\n",
        "                if (i < j):\n",
        "                    s = self.mObserved[i][j] \n",
        "                else:\n",
        "                    s = self.mObserved[j][i] \n",
        "                assert(s <= t)\n",
        "                if (i != j and t > 0):\n",
        "                    self.lstAdjacency[i].add(j)\n",
        "#\n",
        "# TODO: cpmFunc can be countSpokeModel or countMatrixModel\n",
        "#\n",
        "class CInputSet:\n",
        "\n",
        "    def __init__(self, filename, cpmFunc=None):\n",
        "        super().__init__()\n",
        "\n",
        "        listBaits = list()\n",
        "        with open(filename) as fh:\n",
        "            setProteins = set()\n",
        "            for line in fh:\n",
        "                lst = line.rstrip().split(',')\n",
        "                bait = lst[0]\n",
        "                listBaits.append(bait)\n",
        "                setProteins = setProteins.union(set(lst))\n",
        "            print('Number of proteins ' + str(len(setProteins)))\n",
        "            fh.close()\n",
        "\n",
        "        self.aSortedProteins = np.sort(np.array(list(setProteins), dtype='U21'))\n",
        "        bait_inds = np.searchsorted(self.aSortedProteins, np.array(listBaits, dtype='U21'))\n",
        "        \n",
        "        print('Number of purifications ' + str(len(bait_inds)))\n",
        "\n",
        "        nProteins = len(self.aSortedProteins)\n",
        "        self.incidence = np.zeros(shape=(len(bait_inds), nProteins), dtype=int)\n",
        "        with open(filename) as fh:\n",
        "            lineCount = 0\n",
        "            for line in fh:\n",
        "                lst = line.rstrip().split(',')\n",
        "                prey_inds = np.searchsorted(self.aSortedProteins, np.array(lst, dtype='U21'))           \n",
        "                for id in prey_inds:\n",
        "                    self.incidence[lineCount][id] = 1\n",
        "                lineCount += 1\n",
        "            fh.close()\n",
        "            \n",
        "        self.observationG = CountMatrixModel(nProteins, bait_inds, self.incidence)\n",
        "\n",
        "    def writeCluster2File(self, baseName, matQ, indVec):\n",
        "        nRows, nCols = matQ.shape\n",
        "        filePath = baseName + \".tab\"\n",
        "        with open(filePath, \"w\") as fh:\n",
        "            for i in range(nRows):\n",
        "                ind = indVec[i]\n",
        "                fh.write(self.aSortedProteins[i] + '\\t' + str(indVec[i]) + '\\t' + str(max(matQ[ind])) + '\\n')\n",
        "            fh.close()\n",
        "        filePath = baseName + \".csv\"\n",
        "        with open(filePath, \"w\") as fh:\n",
        "            for k in range(nCols):\n",
        "                inds = list(i for i in range(nRows) if indVec[i] == k)\n",
        "                for j in inds:\n",
        "                    protein = self.aSortedProteins[j].split('__')[0] \n",
        "                    fh.write(protein + '\\t')\n",
        "                fh.write('\\n')\n",
        "            fh.close()\n",
        "    \n",
        "    def writeLabel2File(self, indVec):\n",
        "        clusters = {}\n",
        "        for i,k in enumerate(indVec):\n",
        "            if k not in clusters.keys():\n",
        "                clusters[k] = set()\n",
        "            clusters[k].add(i)\n",
        "\n",
        "        with open(\"out.csv\", \"w\") as fh:\n",
        "            for i, k in enumerate(clusters):\n",
        "                for v in clusters[k]:\n",
        "                    protein = self.aSortedProteins[v].split('__')[0] \n",
        "                    fh.write(protein + '\\t')\n",
        "                fh.write('\\n')\n",
        "            fh.close()\n",
        "\n",
        "def clustering(inputSet, Nk, psi, baseName):\n",
        "    fn = 0.8\n",
        "    fp = 0.04\n",
        "    nProteins = inputSet.observationG.nProteins\n",
        "    cmfa = CMeanFieldAnnealing(nProteins, Nk) # default\n",
        "\n",
        "    funcInfer = cmfa\n",
        "\n",
        "    ts = timer()\n",
        "    # alpha = 1e-2\n",
        "    funcInfer.estimate(inputSet.observationG, nProteins, Nk, psi) \n",
        "    te = timer()\n",
        "    print(\"Time running MFA: \", te-ts)\n",
        "    \n",
        "    funcInfer.find_argmax()\n",
        "    \n",
        "    inputSet.writeCluster2File(baseName, funcInfer.mIndicatorQ, funcInfer.indicatorVec)\n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3N_vOiu9BKt",
        "outputId": "ed2a4d2b-6e8e-4344-c995-a23dabe52f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w-qff_K9ZDji"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVoono0R48_e",
        "outputId": "7e0d7a49-6b08-4688-ee03-8af4c77858a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of proteins 2760\n",
            "Number of purifications 2166\n"
          ]
        }
      ],
      "source": [
        "inputSet = CInputSet(\"/content/drive/MyDrive/gavin2006.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCHw7Cnq55bI",
        "outputId": "a283cac4-5aa5-4bba-e0b3-b6acdb3d3b30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 04m 06s]\n",
            "default_objective: 3290654.5996835916\n",
            "\n",
            "Best default_objective So Far: 3168672.4602861702\n",
            "Total elapsed time: 00h 41m 14s\n",
            "2.004948650335344\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from time import time as timer\n",
        "import keras_tuner\n",
        "\n",
        "Nk = 300\n",
        "nProteins = inputSet.observationG.nProteins\n",
        "cmfa = CMeanFieldAnnealing(nProteins, Nk) # default\n",
        "\n",
        "def likelihood_loss(x):\n",
        "    cmfa.estimate(inputSet.observationG, nProteins, Nk, x)  \n",
        "    (fn, fp, p, total_loss) = cmfa.computeErrorRate(inputSet.observationG, nProteins)\n",
        "    regularization_penalty = 0.1*(1.0 + x)      \n",
        "    regularized_loss = total_loss + regularization_penalty # this loss needs to be minimized\n",
        "    print(regularized_loss)\n",
        "    return regularized_loss\n",
        "\n",
        "class RandomSearchTuner(keras_tuner.RandomSearch):\n",
        "    def run_trial(self, trial, *args, **kwargs):\n",
        "        # Get the hp from trial.\n",
        "        hp = trial.hyperparameters\n",
        "        # Define \"x\" as a hyperparameter.\n",
        "        x = hp.Float(\"x\", min_value=1.0, max_value=10.0)\n",
        "        # Return the objective value to minimize.\n",
        "        return likelihood_loss(x)\n",
        "\n",
        "tuner = RandomSearchTuner(\n",
        "    # No hypermodel or objective specified.\n",
        "    max_trials=10,\n",
        "    overwrite=True,\n",
        "    directory=\"my_dir\",\n",
        "    project_name=\"random_search_tune\",\n",
        ")\n",
        "\n",
        "# No need to pass anything to search()\n",
        "# unless you use them in run_trial().\n",
        "tuner.search(\n",
        "    epochs=2,\n",
        "    # Use the TensorBoard callback.\n",
        "    # The logs will be write to \"/tmp/tb_logs\".\n",
        "    callbacks=[keras.callbacks.TensorBoard(\"/tmp/tb_logs\")])\n",
        "print(tuner.get_best_hyperparameters()[0].get(\"x\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBgqBxAAAL8i"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRuiiZ8SLsS2"
      },
      "outputs": [],
      "source": [
        "def clustering(inputSet, k, psi):\n",
        "    nProteins = inputSet.observationG.nProteins\n",
        "    cmfa = CMeanFieldAnnealing(nProteins, k) # default\n",
        "\n",
        "    funcInfer = cmfa        \n",
        "\n",
        "    funcInfer.estimate(inputSet.observationG, nProteins, k, psi)\n",
        "    (fn, fp, errs, f_last) = funcInfer.computeErrorRate(inputSet.observationG, nProteins)\n",
        "    return f_last\n",
        "\n",
        "psi = tuner.get_best_hyperparameters()[0].get(\"x\")\n",
        "max_k = int(inputSet.observationG.nProteins/2)\n",
        "ks = np.arange(100, max_k, 100)\n",
        "ls = []\n",
        "for i, k in enumerate(ks):\n",
        "    ls.append(clustering(inputSet, k, psi))\n",
        "bics = []\n",
        "for i, f in enumerate(ls):\n",
        "    bics.append(np.log(nProteins)*float(ks[i]) + 2.0*ls[i])\n",
        "plt.plot(ks, bics, label=\"{:.2f}\".format(psi))\n",
        "plt.legend(bbox_to_anchor=(1.5, 1.0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXCekphfpjsI"
      },
      "outputs": [],
      "source": [
        "xs = ks[2:]\n",
        "ys = bics[2:]\n",
        "plt.plot(xs, np.min(ys)/ys)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCut2E_xCT4n"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(ks, bics)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8c0PxBygj5a"
      },
      "outputs": [],
      "source": [
        "fig.savefig(\"gavin2006_bic.png\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit ('anaconda3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "96762580dc771c728ac9a1b8aa29a3a420bc09545a8c1a32553175fbb1f6eb2a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}